# Advertised hostname.
druid.host=historical
# The name of the service. This is used as a dimension when emitting metrics and alerts to differentiate between the various services.
druid.service=druid/historical
# This is the port to actually listen on; unless port mapping is used, this will be the same port as is on druid.host.
druid.port=8083

# Number of threads for HTTP requests.
druid.server.http.numThreads=21
# The Jetty max idle time for a connection.
druid.server.http.maxIdleTime=PT2M
# Query timeout
druid.server.http.defaultQueryTimeout=60000

# This specifies a buffer size for the storage of intermediate results. The computation engine in both the Historical and Realtime nodes will use a scratch buffer of this size to do all of their intermediate computations off-heap. Larger values allow for more aggregations in a single pass over the data while smaller values can require more passes depending on the query that is being executed.
druid.processing.buffer.sizeBytes=1073741824
# The number of processing threads to have available for parallel processing of segments. Our rule of thumb is num_cores - 1, which means that even under heavy load there will still be one core available to do background tasks like talking with ZooKeeper and pulling down segments. If only one core is available, this property defaults to the value 1.
druid.processing.numThreads=7

# Segments assigned to a Historical node are first stored on the local file system (in a disk cache) and then served by the Historical node. These locations define where that local cache resides.
druid.segmentCache.locations=[{"path":"var/druid/segment-cache","maxSize"\:130000000000}]
# The maximum number of bytes-worth of segments that the node wants assigned to it. This is not a limit that Historical nodes actually enforces, just a value published to the Coordinator node so it can plan accordingly.
druid.server.maxSize=500000000000
# How frequently to announce segments while segments are loading from cache. Set this value to zero to wait for all segments to be loaded before announcing.
druid.segmentCache.announceIntervalMillis=0

druid.monitoring.monitors=["com.metamx.metrics.JvmMonitor","io.druid.client.cache.CacheMonitor","io.druid.server.metrics.HistoricalMetricsMonitor","io.druid.server.metrics.QueryCountStatsMonitor"]
